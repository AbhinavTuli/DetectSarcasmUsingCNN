# -*- coding: utf-8 -*-
"""Sarcasm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hoeuRbjg3cK3D1kB93CQHaHVwh6yeSWO
"""

import torch
from torchtext import data
from torchtext import datasets
import random
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import en_core_web_sm
import sklearn as sk
import pandas as pd

import re
import glob

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import SVC

import spacy
nlp = spacy.load('en_core_web_sm')
device = torch.device('cpu')
print('Done 1')
TEXT = data.Field(tokenize = 'spacy', batch_first = True)
LABEL = data.LabelField(dtype = torch.float)
print('Done 2')
train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)
train_data, valid_data = train_data.split(random_state = random.seed(1234))
print('Done 3')
TEXT.build_vocab(train_data, max_size = 10000,vectors = "glove.6B.300d", unk_init = torch.Tensor.normal_)
print('Done 4')
class sentimentCNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, dropout, pad_idx):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)
        self.convs = nn.ModuleList([nn.Conv1d(in_channels = embedding_dim, out_channels = 150, kernel_size = fs)for fs in [4,5]])
        self.conv2 = nn.Conv1d(in_channels = 1, out_channels = 100, kernel_size = 3)              
        self.fc1 = nn.Linear(200, 100) 
        #self.fc1_bn=nn.BatchNorm1d(100)
        self.fc2=nn.Linear(100,1)
        #self.dropout = nn.Dropout(dropout)
        
    def forward(self, text):

        embedded = self.embedding(text)        
        embedded = embedded.permute(0, 2, 1)

        sent_len=embedded.size(2)
        padding=3000-sent_len
        batch_size=embedded.size(0)
        torch_padding=torch.zeros(batch_size,300,padding,dtype = embedded.dtype,device = embedded.device)
        lz=[embedded,torch_padding]
        zcat = torch.cat(lz, dim = 2)

        conved = [F.relu(conv(zcat)) for conv in self.convs]

        pooled=[]
        for c in conved:
          pooled.append(F.max_pool1d(c,c.shape[2]))

        pooled = [f.permute(0,2,1) for f in pooled]

        pooled2 = [F.max_pool1d(p, 2) for p in pooled]

        pooled3 = [F.relu(self.conv2(p1)) for p1 in pooled2]

        pooled4=[]
        for c in pooled3:
            pooled4.append(F.max_pool1d(c,c.shape[2]))

        final = torch.cat(pooled4,dim = 1)
        final = final.reshape(batch_size,200)
        full1 = self.fc1(final)
        full2= self.fc2(full1)
        return full2

sentimentModel = sentimentCNN(10002, 300, 0.5, 1)
sentimentModel = sentimentModel.to(device)
print('Done 5')
sentimentModel.load_state_dict(torch.load('sentimentModel.pt',map_location=torch.device('cpu')))

sentimentActivation = {}
def get_sentiactivation(name):
    def hook(model, input, output):
        sentimentActivation[name] = output.detach()
    return hook

for name, layer in sentimentModel.named_modules():
    layer.register_forward_hook(get_sentiactivation(name))

def extract_sentimentfeatures(model,sentence,min_len=5):
  model.eval()
  tokenized = [tok.text for tok in nlp.tokenizer(sentence)]
  if len(tokenized)>3000:
    tokenized=tokenized[:3000]
  if len(tokenized) < min_len:
      tokenized += ['<pad>'] * (min_len - len(tokenized))
  indexed = [TEXT.vocab.stoi[t] for t in tokenized]
  tensor = torch.LongTensor(indexed).to(device)
  tensor = tensor.unsqueeze(0)
  model(tensor)
  return sentimentActivation['fc1']

res = extract_sentimentfeatures(sentimentModel, "Good at being terrible ")

print(res)
print('Done')


#Parse Reviews
#Ironic
data = []
path = 'D:\\BITS\\NNFL\\Project\\DetectSarcasmUsingCNN-master\\Sarcasm\\ARC\\Ironic'
all_files = glob.glob(path + "/*.txt")
for f in all_files:
    with open (f, "r") as myfile:
        inp = myfile.read()
        start = '<REVIEW>'
        end = '</REVIEW>'
        s = inp[inp.index(start)+len(start):inp.index(end)]
        s = s.replace('\n','')
        s = re.escape(s)
        s = s.replace('\\','')
        s = s.strip('\n')
        data.append(s)        
df = pd.DataFrame(data)

df[1] = 1

print(data)
print(df)

print(df.iloc[[0]])

#Regular
data2 = []
path = 'D:\\BITS\\NNFL\\Project\\DetectSarcasmUsingCNN-master\\Sarcasm\\ARC\\Regular'
all_files = glob.glob(path + "/*.txt")
for f in all_files:
    with open (f, "r") as myfile:
        inp = myfile.read()
        start = '<REVIEW>'
        end = '</REVIEW>'
        s = inp[inp.index(start)+len(start):inp.index(end)]
        s = s.replace('\n','')
        s = re.escape(s)
        s = s.replace('\\','')
        s = s.strip('\n')
        data2.append(s)        
df2 = pd.DataFrame(data2)

df2[1] = 0

df = df.append(df2)
print(df)
#Dump Reviews
df.to_csv('test.csv')

#Load Reviews

df = pd.read_csv('test.csv',header=None)
df = df.drop(columns=[0])
print(df)
feat = []

#Extract Features from Reviews

for i in range(1,df.shape[0]):
    x = extract_sentimentfeatures(sentimentModel, df.iloc[i][1])
    px = x.numpy()
    feat.append(px)
featdf = pd.DataFrame(np.array(feat).reshape(-1, 100))
featdf['Sarcasm'] = df[2]
print(featdf)
#Dump Features
featdf.to_csv('parsed.csv')

#Load Features
df = pd.read_csv('parsed.csv',header=None)
df = df.drop(columns=[0])
X = df.iloc[:,:100]
y = df[101]

#Train Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
print(X_train)
print(y_test)

#Support Vector Classifier
# clf = SVC(C=1, kernel='rbf', gamma=1) #The best params by GridSearch

clf = SVC(C=1, kernel='rbf', degree=3, gamma='scale',
        coef0=0.0, shrinking=True, probability=False,
        tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1,
        decision_function_shape='ovr', random_state=None)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print(accuracy_score(y_test, y_pred))


# GridSearch, Include params you want to test for in the grid

# param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf','sigmoid']}
# grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=2)
# grid.fit(X_train,y_train)
# print(grid.best_params_)
# predic = grid.predict(X_test)
# print(classification_report(y_test,predic))
# print(confusion_matrix(y_test, predic))